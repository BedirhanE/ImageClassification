{"cells":[{"cell_type":"markdown","metadata":{"id":"6uQP3ZbC8J5o"},"source":["##### Copyright 2018 The TensorFlow Authors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ckMIh7O7s6D"},"outputs":[],"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vasWnqRgy1H4"},"outputs":[],"source":["#@title MIT License\n","#\n","# Copyright (c) 2017 François Chollet\n","#\n","# Permission is hereby granted, free of charge, to any person obtaining a\n","# copy of this software and associated documentation files (the \"Software\"),\n","# to deal in the Software without restriction, including without limitation\n","# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n","# and/or sell copies of the Software, and to permit persons to whom the\n","# Software is furnished to do so, subject to the following conditions:\n","#\n","# The above copyright notice and this permission notice shall be included in\n","# all copies or substantial portions of the Software.\n","#\n","# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n","# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n","# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n","# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n","# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n","# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n","# DEALINGS IN THE SOFTWARE."]},{"cell_type":"markdown","metadata":{"id":"jYysdyb-CaWM"},"source":["# Image Classification with Convolutional Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"S5Uhzt6vVIB2"},"source":["<table class=\"tfo-notebook-buttons\" align=\"left\">\n","  <td>\n","    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/L03_image_classification_with_cnn.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n","  </td>\n","  <td>\n","    <a target=\"_blank\" href=\"https://github.com/tensorflow/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/L03_image_classification_with_cnn.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n","  </td>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"FbVhjPpzn6BM"},"source":["In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts."]},{"cell_type":"markdown","metadata":{"id":"H0tMfX2vR0uD"},"source":["## Install and import\n","\n","We'll need [TensorFlow Datasets](https://www.tensorflow.org/datasets/), an API that simplifies downloading and accessing datasets, and provides several sample datasets to work with. We're also using a few helper libraries."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5HDhfftMGc_i"},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uusvhUp9Gg37"},"outputs":[],"source":["# Import TensorFlow Datasets\n","import tensorflow_datasets as tfds\n","tfds.disable_progress_bar()\n","\n","# Helper libraries\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXZ44qIaG0Ru"},"outputs":[],"source":["import logging\n","logger = tf.get_logger()#uygulama çalışma zamanını kaydetmek ve görünütlemek için kullanıyoruz.\n","logger.setLevel(logging.ERROR)#logger ın hata seviyesini belirleme işlemi için kullandım.\n"]},{"cell_type":"markdown","metadata":{"id":"yR0EdgrLCaWR"},"source":["## Import the Fashion MNIST dataset"]},{"cell_type":"markdown","metadata":{"id":"DLdCchMdCaWQ"},"source":["This guide uses the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 $\\times$ 28 pixels), as seen here:\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n","         alt=\"Fashion MNIST sprite\" width=\"600\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset—often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n","\n","This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n","\n","We will use 60,000 images to train and validate the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, using the [Datasets](https://www.tensorflow.org/datasets) API:"]},{"cell_type":"markdown","source":[" 10 kategoriden oluşan  70.000 gri tonlamalı görüntü içeren Fashion MNIST veri kümesini kullandım.\n","\n"," fashion_mnist dataseti yükleme ve bu veri kümesini eğitim doğrulama ve test setlerine bölme işlemi."],"metadata":{"id":"U1XyJeUDFero"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MqDQO0KCaWS"},"outputs":[],"source":["dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True, split=['train[:90%]','train[90%:]', 'test'])\n","train_dataset, validation_dataset, test_dataset = dataset\n","\n","#Veri kümesi içeriği ni öğrenmek için\n","print(dataset)"]},{"cell_type":"markdown","metadata":{"id":"t9FDsUlxCaWW"},"source":["Notice that this time we've added a split to the training dataset, reserving 10% for use during validation.\n","\n","* The model is trained using `train_dataset`.\n","* The model validates as it is being trained using the `validation_dataset`\n","* The model is tested against `test_dataset`.\n","\n","The images are 28 $\\times$ 28 arrays, with pixel values in the range `[0, 255]`. The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* of clothing the image represents:\n","\n","<table>\n","  <tr>\n","    <th>Label</th>\n","    <th>Class</th>\n","  </tr>\n","  <tr>\n","    <td>0</td>\n","    <td>T-shirt/top</td>\n","  </tr>\n","  <tr>\n","    <td>1</td>\n","    <td>Trouser</td>\n","  </tr>\n","    <tr>\n","    <td>2</td>\n","    <td>Pullover</td>\n","  </tr>\n","    <tr>\n","    <td>3</td>\n","    <td>Dress</td>\n","  </tr>\n","    <tr>\n","    <td>4</td>\n","    <td>Coat</td>\n","  </tr>\n","    <tr>\n","    <td>5</td>\n","    <td>Sandal</td>\n","  </tr>\n","    <tr>\n","    <td>6</td>\n","    <td>Shirt</td>\n","  </tr>\n","    <tr>\n","    <td>7</td>\n","    <td>Sneaker</td>\n","  </tr>\n","    <tr>\n","    <td>8</td>\n","    <td>Bag</td>\n","  </tr>\n","    <tr>\n","    <td>9</td>\n","    <td>Ankle boot</td>\n","  </tr>\n","</table>\n","\n","Each image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjnLH5S2CaWx"},"outputs":[],"source":["class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal',      'Shirt',   'Sneaker',  'Bag',   'Ankle boot']"]},{"cell_type":"markdown","metadata":{"id":"Brm0b_KACaWX"},"source":["### Explore the data\n","\n","Modeli eğitmeden önce veri kümesinin formatını inceleyelim. Aşağıda eğitim setinde 54.000 görüntü, doğrulama setinde 6000 görüntü ve test setinde 10000 görüntü olduğu gösterilmektedir:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MaOTZxFzi48X"},"outputs":[],"source":["num_train_examples = len(train_dataset)\n","num_validation_examples = len(validation_dataset)\n","num_test_examples = len(test_dataset)\n","print(\"Number of training examples: {}\".format(num_train_examples))\n","print(\"Number of validation examples: {}\".format(num_validation_examples))\n","print(\"Number of test examples:     {}\".format(num_test_examples))"]},{"cell_type":"markdown","metadata":{"id":"ES6uQoLKCaWr"},"source":["## Preprocess the data\n","\n","The value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets."]},{"cell_type":"markdown","source":["Görüntü verilerindeki her pikselin değeri [0,255] aralığında bir tamsayıdır. Modelin düzgün çalışması için bu değerlerin [0,1] aralığına normalize edilmesi gerekir. Bu yüzden burada bir normalleştirme fonksiyonu oluşturuyoruz ve ardından bunu test ve eğitim veri kümelerindeki her bir görüntüye uyguluyoruz."],"metadata":{"id":"IkWqjOWfAWH5"}},{"cell_type":"markdown","source":["#Normalzasyon işlemi"],"metadata":{"id":"JA81yryXcufS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nAsH3Zm-76pB"},"outputs":[],"source":["def normalize(images, labels):\n","  images = tf.cast(images, tf.float64)\n","  images /= 255\n","  return images, labels\n","\n","\n","train_dataset =  train_dataset.map(normalize)\n","validation_dataset = validation_dataset.map(normalize)\n","test_dataset  =  test_dataset.map(normalize)\n","\n","\n","train_dataset =  train_dataset.cache()\n","validation_dataset = validation_dataset.cache()\n","test_dataset  =  test_dataset.cache()"]},{"cell_type":"markdown","metadata":{"id":"lIQbEiJGXM-q"},"source":["### Explore the processed data\n","\n","Let's plot an image to see what it looks like."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oSzE9l7PjHx0"},"outputs":[],"source":["\n","for image, label in test_dataset.take(10):#veri kümesinden örnek resimleri almak için.\n","  #break\n","  image = image.numpy().reshape((28,28))\n","  #image = image.numpy().res((30,30))\n","\n","\n","  plt.figure()\n","  plt.imshow(image, cmap=plt.cm.binary)\n","  plt.colorbar()\n","  plt.grid(False)\n","  plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Ee638AlnCaWz"},"source":["Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network."]},{"cell_type":"markdown","source":["Eğitim setinden ilk 50 görüntüyü görüntüleyin ve her görüntünün altında sınıf adını görüntüleyin. Verilerin doğru formatta olduğunu doğrulayın ve ağı oluşturup eğitmeye hazırız."],"metadata":{"id":"kVW570ELDfk6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZTImqg_CaW1"},"outputs":[],"source":["plt.figure(figsize=(20,20)) #datasetimdeki figure size belirleme\n","i = 0\n","for (image, label) in test_dataset.take(100): #test veri setinden örnek alma işlemi.\n","    image = image.numpy().reshape((28,28))\n","\n","    plt.subplot(10,10,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)#grafi üzerindeki ızgarayı kapatır.\n","    plt.imshow(image, cmap=plt.cm.binary)\n","    #plt.imshow(image, cmap=plt.cm.gray)#gri tonlamalı hali\n","\n","    plt.xlabel(class_names[label])\n","    #plt.title(class_names[label]) #yukarıdaki kodla aynı işlevi görüyor\n","    i += 1\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"59veuiEZCaW4"},"source":["## Build the model\n","\n","Building the neural network requires configuring the layers of the model, then compiling the model."]},{"cell_type":"markdown","metadata":{"id":"Gxg1XGm0eOBy"},"source":["### Setup the layers\n","\n","The basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n","\n","Much of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training.\n","\n","For this exercise, we'll be using two new layers, the Convolution layer (`tf.keras.layers.Conv2D`) and the Max Pooling layer (`tf.keras.layers.MaxPool2D`). Refer to the slides and official documentation on how to use these layers:\n","\n","* [tf.keras.layers.Conv2D reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)\n","* [tf.keras.layers.MaxPool2D reference](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)\n","\n","**Our network layers are:**\n","* 2D Convolution layer - 32 filters, 3x3 kernel, ReLU activation, padding with same values\n","* Max pooling layer - 2x2 kernel, 2 stride\n","* 2D Convolution layer - 64 filters, 3x3 kernel, ReLU activation, padding with same values\n","* Max pooling layer - 2x2 kernel, 2 stride\n","* Flatten layer\n","* Dense layer - 128 nodes output, ReLU activation\n","* Dense layer - 10 nodes output, Softmax activation"]},{"cell_type":"markdown","source":["#Tensorflow ile yazılmış sinir ağı modelim"],"metadata":{"id":"3UrKxWuXa0l5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S8dMETjW_EaN"},"outputs":[],"source":["\"\"\"#raise Exception('This cell has a TODO task! Please complete the TODO task and remove the raise Exception statement.')\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),  # Giriş katmanı (28x28 piksel görüntüler için)\n","    tf.keras.layers.Dense(128, activation='relu'),  # Tam bağlantılı (Dense) katman,tüm nöronlar birbirlerine bağlıdır.\n","    tf.keras.layers.Dropout(0.2),  # Dropout katmanı,aşırı öğrenmeyi önlemek için kullanılır.\n","    #Rastgele seçilen nöronları devre dışı bırakarak (drop) modelin genelleştirmesini artırır.\n","    tf.keras.layers.Dense(10, activation='softmax')  # Çıkış katmanı\n","])\n","\"\"\"\n","\n","\n"]},{"cell_type":"markdown","source":["#CNN MODEL ÖRNEĞİ"],"metadata":{"id":"Tx6XpP50cKGC"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28,1)),\n","    # Bu katman, 32 adet 3x3 filtre kullanarak gelen görüntü üzerinde konvolüsyon işlemi uygular ve ReLU aktivasyon fonksiyonunu kullanır.\n","    tf.keras.layers.MaxPooling2D((2, 2)),#Bu işlem görüntü boyutunu azaltmaya yardımcı olur.\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n"],"metadata":{"id":"Z909DhjFcATi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"enhrZgvr_EaN"},"source":["#### Exercise 3.1 Solution\n","\n","The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E3.1.ipynb)"]},{"cell_type":"code","source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n","                           input_shape=(28, 28, 1)),\n","    tf.keras.layers.MaxPool2D((2, 2), strides=2),\n","    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n","    tf.keras.layers.MaxPool2D((2, 2), strides=2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","print(model)"],"metadata":{"id":"oj2ixjlojzer"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gut8A_7rCaW6"},"source":["### Exercise 3.2 Compile the model with `Model.compile`\n","\n","Before the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n","\n","**Compile the model below with the following settings**\n","* *Loss function* — SparseCategoricalCrossentropy\n","* *Optimizer* — Adam\n","* *Metrics* — accuracy\n","\n","Refer to the [official documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) if you've forgotten the function."]},{"cell_type":"code","source":["#VERİ SETİ HAZIRLAMA İŞLEMİ\n","BATCH_SIZE = 64 #tek seferde kaçtane resmi alıp işleme soksun karar veriyorum\n","train_dataset = train_dataset.cache().shuffle(num_train_examples).batch(BATCH_SIZE)\n","validation_dataset = validation_dataset.cache().batch(BATCH_SIZE)\n","test_dataset = test_dataset.cache().batch(BATCH_SIZE)"],"metadata":{"id":"0Hz3VZH-CQa2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ew3O9_Mq_EaN"},"outputs":[],"source":["\n","import tensorflow as tf\n","\n","# Modeli tanımla\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Input(shape=(28, 28, 1)),  # Giriş boyutunu belirt\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.BatchNormalization(),\n","    tf.keras.layers.Dense(256, activation='relu'),\n","    tf.keras.layers.Dropout(0.3),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","])\n","\n","# Modeli derle\n","model.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","# Modeli eğitme işlemi\n","model.fit(train_dataset, epochs=10, validation_data=validation_dataset)\n"]},{"cell_type":"markdown","source":["Bu çıktı, modelin eğitim sürecini gösteren eğitim geçmişi (history) bilgilerini içerir. Her bir epoch için eğitim seti ve doğrulama seti üzerindeki kayıplar (loss) ve doğruluk (accuracy) değerleri listelenmiştir. Şimdi çıktıdaki bazı önemli bilgileri açıklayalım:\n","\n","Epoch Sayısı: İterasyonları temsil eder. Her epoch, modelin tüm eğitim verilerini bir kez geçmesini ifade eder.\n","\n","Iterasyon Sayısı (Steps): Her bir epoch içindeki toplam iterasyon sayısını belirtir. Bu, veri setinin boyutuna ve kullanılan mini-batch boyutuna bağlıdır.\n","\n","Eğitim Loss ve Accuracy: Modelin eğitim setindeki performansını gösterir. \"loss\" değeri, modelin eğitim setindeki kaybını, \"accuracy\" değeri ise doğruluğunu ifade eder.\n","\n","Doğrulama (Validation) Loss ve Accuracy: Modelin doğrulama setindeki performansını gösterir. Doğrulama seti, modelin eğitim sırasında görmediği, genellikle ayrılmış bir veri setidir. \"val_loss\" ve \"val_accuracy\" değerleri, modelin doğrulama setindeki kaybını ve doğruluğunu gösterir.\n","\n","Çıktıdaki örneklerde, her epoch sonunda modelin eğitim ve doğrulama setindeki performansının nasıl değiştiği görülmektedir. Özellikle eğitim setindeki kayıp ve doğruluk değerlerinin doğrulama setindeki değerlere kıyasla nasıl değiştiğine dikkat edilmelidir. Ayrıca, modelin genelleme yeteneğini değerlendirmek için doğrulama setindeki performansına odaklanmak önemlidir. Eğitim setindeki yüksek doğruluk, modelin verileri \"ezberleme\" eğiliminde olduğunu gösterebilir, bu nedenle doğrulama setindeki performans daha güvenilir bir gösterge olabilir."],"metadata":{"id":"Nr2a44tYpGai"}},{"cell_type":"markdown","source":["Model eğitme işlemi"],"metadata":{"id":"p06zZo9iQZSq"}},{"cell_type":"markdown","metadata":{"id":"2YZBBbW6_EaN"},"source":["#### Exercise 3.2 Solution\n","\n","The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E3.2.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"qKF6uW-BCaW-"},"source":["## Exercise 3.3 Train the model with `Model.fit`\n","\n","First, we define the iteration behavior for the train dataset:\n","1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n","2. The `dataset.shuffle(dataset_size)` randomizes the order so our model cannot learn anything from the order of the examples.\n","3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n","\n","Training is performed by calling the `model.fit` method:\n","1. Feed the training data to the model using `train_dataset`.\n","2. The model learns to associate images and labels.\n","3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n"]},{"cell_type":"markdown","metadata":{"id":"FGBs-pBU_EaO"},"source":["Start training the model in the code box below for **10 epochs**.\n","\n","Refer to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model) if you've forgotten the function."]},{"cell_type":"markdown","metadata":{"id":"W3ZVOhugCaXA"},"source":["As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.97 (or 97%) on the training data."]},{"cell_type":"markdown","metadata":{"id":"ZKFi9xcG_EaO"},"source":["### Exercise 3.3 Solution\n","\n","The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E3.3.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"oEw4bZgGCaXB"},"source":["## Exercise 3.4 Evaluate accuracy with `Model.evaluate`\n","\n","Next, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy.\n","\n","Refer to the [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model) on how to use the function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RK80r1n0_EaO"},"outputs":[],"source":["\"\"\"raise Exception('This cell has a TODO task! Please complete the TODO task and remove the raise Exception statement.')\n","\n","# TODO - Evaluate the model\"\"\"\n","\n","\n","# Evaluate the model on the test dataset\n","test_loss, test_accuracy = model.evaluate(test_dataset)\n","\n","# Print the test accuracy\n","print(f'Test Accuracy: {test_accuracy}')\n"]},{"cell_type":"markdown","metadata":{"id":"yWfgsmVXCaXG"},"source":["As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down."]},{"cell_type":"markdown","metadata":{"id":"hg_gMztD_EaS"},"source":["### Exercise 3.4 Solution\n","\n","The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E3.4.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"xsoS7CPDCaXH"},"source":["## Make predictions and explore\n","\n","With the model trained, we can use it to make predictions about some images."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ccoz4conNCpl"},"outputs":[],"source":["for test_images, test_labels in test_dataset.take(1):\n","  test_images = test_images.numpy()\n","  test_labels = test_labels.numpy()\n","  predictions = model.predict(test_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl91RPhdCaXI"},"outputs":[],"source":["predictions.shape\n"]},{"cell_type":"markdown","metadata":{"id":"x9Kk1voUCaXJ"},"source":["Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3DmJEUinCaXK"},"outputs":[],"source":["predictions[0]"]},{"cell_type":"markdown","metadata":{"id":"-hw1hgeSCaXN"},"source":["A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsqenuPnCaXO"},"outputs":[],"source":["np.argmax(predictions[0])"]},{"cell_type":"markdown","metadata":{"id":"E51yS7iCCaXO"},"source":["So the model is usually most confident that this image is a coat, or `class_names[4]`. Let's check the label:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sd7Pgsu6CaXP"},"outputs":[],"source":["test_labels[0]"]},{"cell_type":"markdown","metadata":{"id":"ygh2yYC972ne"},"source":["We can graph this to look at the full set of 10 class predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvYmmrpIy6Y1"},"outputs":[],"source":["def plot_image(i, predictions_array, true_labels, images):\n","  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","\n","  plt.imshow(img[...,0], cmap=plt.cm.binary)\n","\n","  predicted_label = np.argmax(predictions_array)\n","  if predicted_label == true_label:\n","    color = 'blue'\n","  else:\n","    color = 'red'\n","\n","  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n","                                100*np.max(predictions_array),\n","                                class_names[true_label]),\n","                                color=color)\n","\n","def plot_value_array(i, predictions_array, true_label):\n","  predictions_array, true_label = predictions_array[i], true_label[i]\n","  plt.grid(False)\n","  plt.xticks([])\n","  plt.yticks([])\n","  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n","  plt.ylim([0, 1])\n","  predicted_label = np.argmax(predictions_array)\n","\n","  thisplot[predicted_label].set_color('red')\n","  thisplot[true_label].set_color('blue')"]},{"cell_type":"markdown","metadata":{"id":"d4Ov9OFDMmOD"},"source":["Let's look at the 0th image, predictions, and prediction array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HV5jw-5HwSmO"},"outputs":[],"source":["i = 0\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions, test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions, test_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ko-uzOufSCSe"},"outputs":[],"source":["i = 12\n","plt.figure(figsize=(6,3))\n","plt.subplot(1,2,1)\n","plot_image(i, predictions, test_labels, test_images)\n","plt.subplot(1,2,2)\n","plot_value_array(i, predictions, test_labels)"]},{"cell_type":"markdown","metadata":{"id":"kgdvGD52CaXR"},"source":["Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQlnbqaw2Qu_"},"outputs":[],"source":["# Plot the first X test images, their predicted label, and the true label\n","# Color correct predictions in blue, incorrect predictions in red\n","num_rows = 5\n","num_cols = 3\n","num_images = num_rows*num_cols\n","plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n","for i in range(num_images):\n","  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n","  plot_image(i, predictions, test_labels, test_images)\n","  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n","  plot_value_array(i, predictions, test_labels)\n"]},{"cell_type":"markdown","metadata":{"id":"R32zteKHCaXT"},"source":["Finally, use the trained model to make a prediction about a single image."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yRJ7JU7JCaXT"},"outputs":[],"source":["# Grab an image from the test dataset\n","img = test_images[0]\n","\n","print(img.shape)"]},{"cell_type":"markdown","metadata":{"id":"vz3bVp21CaXV"},"source":["`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDFh5yF_CaXW"},"outputs":[],"source":["# Add the image to a batch where it's the only member.\n","img = np.array([img])\n","\n","print(img.shape)"]},{"cell_type":"markdown","metadata":{"id":"EQ5wLTkcCaXY"},"source":["Now predict the image:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o_rzNSdrCaXY"},"outputs":[],"source":["predictions_single = model.predict(img)\n","\n","print(predictions_single)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Ai-cpLjO-3A"},"outputs":[],"source":["plot_value_array(0, predictions_single, test_labels)\n","_ = plt.xticks(range(10), class_names, rotation=45)"]},{"cell_type":"markdown","metadata":{"id":"cU1Y2OAMCaXb"},"source":["`model.predict` returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tRmdq_8CaXb"},"outputs":[],"source":["np.argmax(predictions_single[0])"]},{"cell_type":"markdown","metadata":{"id":"YFc2HbEVCaXd"},"source":["And, as before, the model predicts a label of 4 (coat)."]},{"cell_type":"markdown","metadata":{"id":"-KtnHECKZni_"},"source":["# Exercise 3.5\n","\n","Experiment with different models and see how the accuracy results differ. In particular change the following parameters:\n","*   Set training epochs set to 1\n","*   Number of neurons in the Dense layer following the Flatten one. For example, go really low (e.g. 10) in ranges up to 512 and see how accuracy changes\n","*   Add additional Dense layers between the Flatten and the final Dense(10), experiment with different units in these layers\n","*   Don't normalize the pixel values, and see the effect that has"]},{"cell_type":"markdown","metadata":{"id":"IncLGIlE_EaX"},"source":["\n","# Exercise 3.6 - CIFAR-10 Dataset with CNNs\n","\n","Let's apply what we've learned to another dataset.The [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n","\n","As our input is a colour image, we have now 3 values per pixel. When flattened, our input array is is 3072 long ($32\\times32\\times3$).\n","\n","* What happens when you use the same network as above?\n","* What is the best accuracy that you can achieve?"]},{"cell_type":"markdown","metadata":{"id":"9mnDymHL_EaX"},"source":["Like in the previous lab, download, extract and load the dataset.\n","\n","The extracted folder `cifar-10-batches-py` contains (in Python's pickle format):\n","* Training dataset: `data_batch_1 - 5`\n","* Test dataset: `test_batch`\n","* Dataset metadata: `batches.meta`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpjkLo5f_EaX"},"outputs":[],"source":["import os\n","import glob\n","\n","# Download the data\n","_URL = 'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz'\n","zip_dir = tf.keras.utils.get_file('cifar-10-python.tar.gz', origin=_URL, extract=True)\n","\n","# Get the data and meta file names\n","data_dir = os.path.join(os.path.dirname(zip_dir), 'cifar-10-batches-py')\n","train_files = glob.glob(os.path.join(data_dir,\"data_batch_*\"))\n","test_file = os.path.join(data_dir,\"test_batch\")\n","meta_file = os.path.join(data_dir,\"batches.meta\")\n","\n","def unpickle(file):\n","    import pickle\n","    with open(file, 'rb') as fo:\n","        dict = pickle.load(fo, encoding='bytes')\n","    return dict\n","\n","def build_dataset(files):\n","    x = []\n","    y = []\n","    for file in files:\n","        dict = unpickle(file)\n","        for image in dict[b'data']:\n","            # Image in the dataset is stored as a 3072 length 1D array\n","            x.append(image)\n","        for label in dict[b'labels']:\n","            y.append(label)\n","\n","    return tf.data.Dataset.from_tensor_slices((x,y))\n","\n","# Build the training dataset\n","train_dataset  = build_dataset(train_files)\n","\n","# Build the testing dataset\n","test_dataset = build_dataset([test_file])\n","\n","# Get the metadata\n","meta = unpickle(meta_file)"]},{"cell_type":"markdown","metadata":{"id":"kS4Cwe1W_EaX"},"source":["**Now that we've got a dataset, use what you've learned in this lab to build a CNN model for classifying these images.**\n","* Don't forget to pre-process your data\n","    * You'll want change the shape of the input image from 1D to a 3D array inside your mapping function (hint: [use the reshape function](https://www.tensorflow.org/api_docs/python/tf/reshape))\n","    * The image is stored as `[colour channel, width, height]`, you'll need to change this odering to `[width, height, colour channel]` (hint: [use the transpose function](https://www.tensorflow.org/api_docs/python/tf/transpose))\n","* Remember to check our input shape as it's different from the fashion mnist dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rxMxq2z_EaX"},"outputs":[],"source":["# Perform dataset normalisation and configure our dataset\n","def reshape_and_normalize(images, labels):\n","    # Convert from 1D array to 3D array of [3, 32, 32]\n","    # the image is stored as [colour channel, width, height]\n","    images = tf.reshape(images, (3, 32, 32))\n","    # Swap from [colour channel, width, height] to [width, height, colour channel]\n","    images = tf.transpose(images, (1, 2, 0))\n","    # Convert to float32\n","    images = tf.cast(images, tf.float32)\n","    # Normalize\n","    images /= 255\n","    return images, labels\n","\n","\n","train_dataset =  train_dataset.map(reshape_and_normalize)\n","test_dataset  =  test_dataset.map(reshape_and_normalize)\n","\n","num_train_examples = 50000\n","BATCH_SIZE = 32\n","train_dataset = train_dataset.cache().shuffle(num_train_examples).batch(BATCH_SIZE)\n","test_dataset = test_dataset.cache().batch(BATCH_SIZE)\n","\n","# Create and fit our model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(32, (3,3), padding='same', activation=tf.nn.relu,\n","                           input_shape=(32, 32, 3)),\n","    tf.keras.layers.MaxPool2D((2, 2), strides=2),\n","    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation=tf.nn.relu),\n","    tf.keras.layers.MaxPool2D((2, 2), strides=2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(10)\n","])\n","model.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy', 'mse'])\n","model.fit(train_dataset,\n","          epochs=2)\n","\n","# Evaluate the model with the test dataset\n","model.evaluate(test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"qvatBlgt_EaY"},"source":["### Exercise 3.6 Solution\n","\n","The solution for the exercise can be found [here](https://colab.research.google.com/github/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/solutions/E3.6.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"ZfnZTFLs_EaY"},"source":["# Notice\n","\n","Remember to enable GPU to make everything run faster (Runtime -> Change runtime type -> Hardware accelerator -> GPU).\n","Also, if you run into trouble, simply reset the entire environment and start from the beginning:\n","*   Edit -> Clear all outputs\n","*   Runtime -> Reset all runtimes"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"https://github.com/rses-dl-course/rses-dl-course.github.io/blob/master/notebooks/python/L03_image_classification_with_cnn.ipynb","timestamp":1703491165243}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}